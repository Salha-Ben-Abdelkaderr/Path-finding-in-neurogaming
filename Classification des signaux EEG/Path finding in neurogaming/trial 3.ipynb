{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43a0f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers , models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1f8bc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>time</th>\n",
       "      <th>label</th>\n",
       "      <th>epoch</th>\n",
       "      <th>EEG-Fz</th>\n",
       "      <th>EEG-0</th>\n",
       "      <th>EEG-1</th>\n",
       "      <th>EEG-2</th>\n",
       "      <th>EEG-3</th>\n",
       "      <th>EEG-4</th>\n",
       "      <th>...</th>\n",
       "      <th>EEG-8</th>\n",
       "      <th>EEG-9</th>\n",
       "      <th>EEG-10</th>\n",
       "      <th>EEG-11</th>\n",
       "      <th>EEG-12</th>\n",
       "      <th>EEG-13</th>\n",
       "      <th>EEG-14</th>\n",
       "      <th>EEG-Pz</th>\n",
       "      <th>EEG-15</th>\n",
       "      <th>EEG-16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>tongue</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.681412</td>\n",
       "      <td>2.245496</td>\n",
       "      <td>-0.158350</td>\n",
       "      <td>1.163765</td>\n",
       "      <td>-1.523659</td>\n",
       "      <td>-0.575267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758116</td>\n",
       "      <td>3.441785</td>\n",
       "      <td>0.305517</td>\n",
       "      <td>1.137473</td>\n",
       "      <td>-1.275763</td>\n",
       "      <td>-2.898359</td>\n",
       "      <td>0.656704</td>\n",
       "      <td>-2.010063</td>\n",
       "      <td>-1.613804</td>\n",
       "      <td>-1.942455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>tongue</td>\n",
       "      <td>8</td>\n",
       "      <td>0.420417</td>\n",
       "      <td>0.587559</td>\n",
       "      <td>1.650510</td>\n",
       "      <td>0.970672</td>\n",
       "      <td>1.505904</td>\n",
       "      <td>0.891796</td>\n",
       "      <td>...</td>\n",
       "      <td>1.541586</td>\n",
       "      <td>-0.071620</td>\n",
       "      <td>0.258909</td>\n",
       "      <td>-1.448198</td>\n",
       "      <td>0.142472</td>\n",
       "      <td>-1.968405</td>\n",
       "      <td>-1.733655</td>\n",
       "      <td>-2.935578</td>\n",
       "      <td>-3.125256</td>\n",
       "      <td>-4.674610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>tongue</td>\n",
       "      <td>8</td>\n",
       "      <td>0.551365</td>\n",
       "      <td>1.499758</td>\n",
       "      <td>0.121302</td>\n",
       "      <td>2.859433</td>\n",
       "      <td>2.613414</td>\n",
       "      <td>4.636026</td>\n",
       "      <td>...</td>\n",
       "      <td>2.649097</td>\n",
       "      <td>-2.137938</td>\n",
       "      <td>-1.612096</td>\n",
       "      <td>-1.610218</td>\n",
       "      <td>-0.410173</td>\n",
       "      <td>-0.274957</td>\n",
       "      <td>-4.776535</td>\n",
       "      <td>-5.099551</td>\n",
       "      <td>-2.798995</td>\n",
       "      <td>-5.862021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>tongue</td>\n",
       "      <td>8</td>\n",
       "      <td>3.054916</td>\n",
       "      <td>-1.807238</td>\n",
       "      <td>1.843603</td>\n",
       "      <td>2.286812</td>\n",
       "      <td>5.995872</td>\n",
       "      <td>6.651295</td>\n",
       "      <td>...</td>\n",
       "      <td>6.031554</td>\n",
       "      <td>-5.249621</td>\n",
       "      <td>-2.672998</td>\n",
       "      <td>-3.452370</td>\n",
       "      <td>0.189081</td>\n",
       "      <td>1.593829</td>\n",
       "      <td>-6.081577</td>\n",
       "      <td>-5.476860</td>\n",
       "      <td>-2.932163</td>\n",
       "      <td>-6.874095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>tongue</td>\n",
       "      <td>8</td>\n",
       "      <td>2.506710</td>\n",
       "      <td>-2.453101</td>\n",
       "      <td>0.221178</td>\n",
       "      <td>0.127278</td>\n",
       "      <td>4.519931</td>\n",
       "      <td>6.249573</td>\n",
       "      <td>...</td>\n",
       "      <td>7.827097</td>\n",
       "      <td>-5.309546</td>\n",
       "      <td>-2.488783</td>\n",
       "      <td>-3.707608</td>\n",
       "      <td>1.447515</td>\n",
       "      <td>4.268278</td>\n",
       "      <td>-4.383690</td>\n",
       "      <td>-4.218426</td>\n",
       "      <td>-1.331932</td>\n",
       "      <td>-5.322692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57883</th>\n",
       "      <td>1</td>\n",
       "      <td>0.684</td>\n",
       "      <td>left</td>\n",
       "      <td>602</td>\n",
       "      <td>-22.074154</td>\n",
       "      <td>-11.489719</td>\n",
       "      <td>-9.314989</td>\n",
       "      <td>-8.518715</td>\n",
       "      <td>-6.629442</td>\n",
       "      <td>-10.866221</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.208619</td>\n",
       "      <td>3.177499</td>\n",
       "      <td>5.344716</td>\n",
       "      <td>3.831044</td>\n",
       "      <td>6.110942</td>\n",
       "      <td>6.047090</td>\n",
       "      <td>10.841636</td>\n",
       "      <td>13.959124</td>\n",
       "      <td>14.028611</td>\n",
       "      <td>19.901132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57884</th>\n",
       "      <td>1</td>\n",
       "      <td>0.688</td>\n",
       "      <td>left</td>\n",
       "      <td>602</td>\n",
       "      <td>-24.568827</td>\n",
       "      <td>-12.128923</td>\n",
       "      <td>-11.760834</td>\n",
       "      <td>-9.890342</td>\n",
       "      <td>-8.587006</td>\n",
       "      <td>-11.798394</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.068527</td>\n",
       "      <td>4.198450</td>\n",
       "      <td>6.219184</td>\n",
       "      <td>6.805121</td>\n",
       "      <td>6.594785</td>\n",
       "      <td>6.726245</td>\n",
       "      <td>11.667276</td>\n",
       "      <td>14.540623</td>\n",
       "      <td>14.317140</td>\n",
       "      <td>20.970911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57885</th>\n",
       "      <td>1</td>\n",
       "      <td>0.692</td>\n",
       "      <td>left</td>\n",
       "      <td>602</td>\n",
       "      <td>-25.776214</td>\n",
       "      <td>-8.892950</td>\n",
       "      <td>-10.233846</td>\n",
       "      <td>-10.316478</td>\n",
       "      <td>-10.233846</td>\n",
       "      <td>-11.785078</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.934116</td>\n",
       "      <td>6.067236</td>\n",
       "      <td>6.623125</td>\n",
       "      <td>6.525469</td>\n",
       "      <td>5.436226</td>\n",
       "      <td>4.005187</td>\n",
       "      <td>12.950124</td>\n",
       "      <td>14.016830</td>\n",
       "      <td>12.670301</td>\n",
       "      <td>18.347509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57886</th>\n",
       "      <td>1</td>\n",
       "      <td>0.696</td>\n",
       "      <td>left</td>\n",
       "      <td>602</td>\n",
       "      <td>-22.473657</td>\n",
       "      <td>-6.762268</td>\n",
       "      <td>-7.077773</td>\n",
       "      <td>-9.943609</td>\n",
       "      <td>-9.323867</td>\n",
       "      <td>-14.097755</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.270231</td>\n",
       "      <td>8.393230</td>\n",
       "      <td>8.558495</td>\n",
       "      <td>6.117088</td>\n",
       "      <td>3.270033</td>\n",
       "      <td>-0.260616</td>\n",
       "      <td>12.492915</td>\n",
       "      <td>13.217825</td>\n",
       "      <td>9.869342</td>\n",
       "      <td>17.060222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57887</th>\n",
       "      <td>1</td>\n",
       "      <td>0.700</td>\n",
       "      <td>left</td>\n",
       "      <td>602</td>\n",
       "      <td>-22.389317</td>\n",
       "      <td>-2.429882</td>\n",
       "      <td>-5.137965</td>\n",
       "      <td>-7.515519</td>\n",
       "      <td>-10.313746</td>\n",
       "      <td>-14.111072</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.994485</td>\n",
       "      <td>9.942414</td>\n",
       "      <td>7.666272</td>\n",
       "      <td>7.324475</td>\n",
       "      <td>1.938357</td>\n",
       "      <td>-2.422370</td>\n",
       "      <td>12.333114</td>\n",
       "      <td>11.642008</td>\n",
       "      <td>6.731025</td>\n",
       "      <td>15.386749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57888 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       patient   time   label  epoch     EEG-Fz      EEG-0      EEG-1  \\\n",
       "0            1 -0.100  tongue      8  -1.681412   2.245496  -0.158350   \n",
       "1            1 -0.096  tongue      8   0.420417   0.587559   1.650510   \n",
       "2            1 -0.092  tongue      8   0.551365   1.499758   0.121302   \n",
       "3            1 -0.088  tongue      8   3.054916  -1.807238   1.843603   \n",
       "4            1 -0.084  tongue      8   2.506710  -2.453101   0.221178   \n",
       "...        ...    ...     ...    ...        ...        ...        ...   \n",
       "57883        1  0.684    left    602 -22.074154 -11.489719  -9.314989   \n",
       "57884        1  0.688    left    602 -24.568827 -12.128923 -11.760834   \n",
       "57885        1  0.692    left    602 -25.776214  -8.892950 -10.233846   \n",
       "57886        1  0.696    left    602 -22.473657  -6.762268  -7.077773   \n",
       "57887        1  0.700    left    602 -22.389317  -2.429882  -5.137965   \n",
       "\n",
       "           EEG-2      EEG-3      EEG-4  ...     EEG-8     EEG-9    EEG-10  \\\n",
       "0       1.163765  -1.523659  -0.575267  ...  0.758116  3.441785  0.305517   \n",
       "1       0.970672   1.505904   0.891796  ...  1.541586 -0.071620  0.258909   \n",
       "2       2.859433   2.613414   4.636026  ...  2.649097 -2.137938 -1.612096   \n",
       "3       2.286812   5.995872   6.651295  ...  6.031554 -5.249621 -2.672998   \n",
       "4       0.127278   4.519931   6.249573  ...  7.827097 -5.309546 -2.488783   \n",
       "...          ...        ...        ...  ...       ...       ...       ...   \n",
       "57883  -8.518715  -6.629442 -10.866221  ... -2.208619  3.177499  5.344716   \n",
       "57884  -9.890342  -8.587006 -11.798394  ... -4.068527  4.198450  6.219184   \n",
       "57885 -10.316478 -10.233846 -11.785078  ... -4.934116  6.067236  6.623125   \n",
       "57886  -9.943609  -9.323867 -14.097755  ... -6.270231  8.393230  8.558495   \n",
       "57887  -7.515519 -10.313746 -14.111072  ... -9.994485  9.942414  7.666272   \n",
       "\n",
       "         EEG-11    EEG-12    EEG-13     EEG-14     EEG-Pz     EEG-15  \\\n",
       "0      1.137473 -1.275763 -2.898359   0.656704  -2.010063  -1.613804   \n",
       "1     -1.448198  0.142472 -1.968405  -1.733655  -2.935578  -3.125256   \n",
       "2     -1.610218 -0.410173 -0.274957  -4.776535  -5.099551  -2.798995   \n",
       "3     -3.452370  0.189081  1.593829  -6.081577  -5.476860  -2.932163   \n",
       "4     -3.707608  1.447515  4.268278  -4.383690  -4.218426  -1.331932   \n",
       "...         ...       ...       ...        ...        ...        ...   \n",
       "57883  3.831044  6.110942  6.047090  10.841636  13.959124  14.028611   \n",
       "57884  6.805121  6.594785  6.726245  11.667276  14.540623  14.317140   \n",
       "57885  6.525469  5.436226  4.005187  12.950124  14.016830  12.670301   \n",
       "57886  6.117088  3.270033 -0.260616  12.492915  13.217825   9.869342   \n",
       "57887  7.324475  1.938357 -2.422370  12.333114  11.642008   6.731025   \n",
       "\n",
       "          EEG-16  \n",
       "0      -1.942455  \n",
       "1      -4.674610  \n",
       "2      -5.862021  \n",
       "3      -6.874095  \n",
       "4      -5.322692  \n",
       "...          ...  \n",
       "57883  19.901132  \n",
       "57884  20.970911  \n",
       "57885  18.347509  \n",
       "57886  17.060222  \n",
       "57887  15.386749  \n",
       "\n",
       "[57888 rows x 26 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading data\n",
    "data= pd.read_csv('BCICIV_2a_1.csv', delimiter=',')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33e3228d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Define the electrode positions (x, y coordinates)\n",
    "electrode_positions = {\n",
    "    'Fz': (0, 2),\n",
    "    'C3': (-1, 1),\n",
    "    'Cz': (0, 1),\n",
    "    'C4': (1, 1),\n",
    "    'Pz': (0, 0),\n",
    "}\n",
    "\n",
    "# Define the electrodes used for left and right motor imagery classification\n",
    "left_motor_imagery_electrodes = ['C3']\n",
    "right_motor_imagery_electrodes = ['C4']\n",
    "\n",
    "# Step 2: Plot the scalp representation with electrodes\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter([pos[0] for pos in electrode_positions.values()],\n",
    "            [pos[1] for pos in electrode_positions.values()],\n",
    "            s=200, edgecolors='k', c='lightgray', alpha=0.8)\n",
    "plt.axis('equal')\n",
    "\n",
    "# Step 3: Color-code the electrodes used for left and right motor imagery\n",
    "for electrode, pos in electrode_positions.items():\n",
    "    if electrode in left_motor_imagery_electrodes:\n",
    "        plt.scatter(pos[0], pos[1], s=200, color='blue', edgecolors='k')\n",
    "        plt.text(pos[0] + 0.1, pos[1] + 0.1, electrode, fontsize=10, color='blue')\n",
    "    elif electrode in right_motor_imagery_electrodes:\n",
    "        plt.scatter(pos[0], pos[1], s=200, color='red', edgecolors='k')\n",
    "        plt.text(pos[0] + 0.1, pos[1] + 0.1, electrode, fontsize=10, color='red')\n",
    "    else:\n",
    "        plt.text(pos[0] + 0.1, pos[1] + 0.1, electrode, fontsize=10, color='black')\n",
    "\n",
    "plt.title('Electrode Positions for Motor Imagery Classification')\n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "\n",
    "# Step 4: Save the image\n",
    "plt.savefig('electrode_positions.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6dc0be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-0322ee188795>:21: RuntimeWarning: Fiducial point nasion not found, assuming identity unknown to head transformation\n",
      "  layout.plot()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "plot() got an unexpected keyword argument 'picks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-0322ee188795>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mch_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mright_motor_imagery_electrodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'red'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mlayout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpicks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: plot() got an unexpected keyword argument 'picks'"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the electrode positions (x, y, z coordinates)\n",
    "electrode_positions = {\n",
    "    'Fz': [0, 0, 1],\n",
    "    'C3': [-0.5, 0.5, 1],\n",
    "    'Cz': [0, 0.5, 1],\n",
    "    'C4': [0.5, 0.5, 1],\n",
    "    'Pz': [0, 1, 1],\n",
    "}\n",
    "\n",
    "# Define the electrodes used for left and right motor imagery classification\n",
    "left_motor_imagery_electrodes = ['C3']\n",
    "right_motor_imagery_electrodes = ['C4']\n",
    "\n",
    "# Step 2: Create a 3D layout with electrode positions\n",
    "layout = mne.channels.make_dig_montage(ch_pos=dict(zip(electrode_positions.keys(), electrode_positions.values())))\n",
    "\n",
    "# Step 3: Plot the scalp representation with electrodes\n",
    "layout.plot()\n",
    "\n",
    "# Step 4: Color-code the electrodes used for left and right motor imagery\n",
    "for ch_name in layout.ch_names:\n",
    "    color = 'gray'\n",
    "    if ch_name in left_motor_imagery_electrodes:\n",
    "        color = 'blue'\n",
    "    elif ch_name in right_motor_imagery_electrodes:\n",
    "        color = 'red'\n",
    "    layout.plot(picks=[ch_name], show_names=True, color=color)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "573b6948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DigMontage | 0 extras (headshape), 0 HPIs, 3 fiducials, 61 channels>\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File does not exist: C:\\Users\\21694\\Path finding in neurogaming\\easycap-M10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-22f5152e3a18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0msfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m250\u001b[0m  \u001b[1;31m# Assuming a sampling frequency of 250 Hz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mch_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mch_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mmontage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_custom_montage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'easycap-M10'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# You can replace this with your montage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Create the RawArray object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mne\\channels\\montage.py\u001b[0m in \u001b[0;36mread_custom_montage\u001b[1;34m(fname, head_size, coord_frame)\u001b[0m\n\u001b[0;32m   1616\u001b[0m     }\n\u001b[0;32m   1617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1618\u001b[1;33m     \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_check_fname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"read\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmust_exist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1619\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1620\u001b[0m     \u001b[0m_check_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fname\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSUPPORTED_FILE_EXT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-0>\u001b[0m in \u001b[0;36m_check_fname\u001b[1;34m(fname, overwrite, must_exist, name, need_dir, verbose)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mne\\utils\\check.py\u001b[0m in \u001b[0;36m_check_fname\u001b[1;34m(fname, overwrite, must_exist, name, need_dir, verbose)\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mPermissionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{name} does not have read permissions: {fname}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmust_exist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{name} does not exist: {fname}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File does not exist: C:\\Users\\21694\\Path finding in neurogaming\\easycap-M10"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mne\n",
    "easycap_montage = mne.channels.make_standard_montage('easycap-M10')\n",
    "print(easycap_montage)\n",
    "from mne.channels import read_custom_montage\n",
    "import os\n",
    "\n",
    "# Step 1: Read the CSV dataset\n",
    "data = pd.read_csv('BCICIV_2a_1.csv')\n",
    "\n",
    "# Step 2: Create scalp plots and save as images in separate folders\n",
    "for index, row in data.iterrows():\n",
    "    target = row['label']\n",
    "    eeg_data = row[['EEG-Fz', 'EEG-0', 'EEG-1', 'EEG-2', 'EEG-3', 'EEG-4', 'EEG-5', 'EEG-C3', 'EEG-6', 'EEG-Cz', 'EEG-7',\n",
    "                    'EEG-C4', 'EEG-8', 'EEG-9', 'EEG-10', 'EEG-11', 'EEG-12', 'EEG-13', 'EEG-14', 'EEG-Pz', 'EEG-15',\n",
    "                    'EEG-16']].values\n",
    "\n",
    "    # Convert the EEG data to a 2D array (n_channels x n_times)\n",
    "    eeg_data = eeg_data.reshape(1, -1)\n",
    "\n",
    "    # Create a dummy info object with channel names and montage\n",
    "    ch_names = ['Fz', '0', '1', '2', '3', '4', '5', 'C3', '6', 'Cz', '7', 'C4', '8', '9', '10', '11', '12', '13', '14',\n",
    "                'Pz', '15', '16']\n",
    "    ch_types = ['eeg'] * len(ch_names)\n",
    "    sfreq = 250  # Assuming a sampling frequency of 250 Hz\n",
    "    info = mne.create_info(ch_names=ch_names, ch_types=ch_types, sfreq=sfreq)\n",
    "    montage = read_custom_montage('easycap-M10')  # You can replace this with your montage\n",
    "\n",
    "    # Create the RawArray object\n",
    "    raw_array = mne.io.RawArray(eeg_data, info, verbose='error')\n",
    "    raw_array.set_montage(montage)\n",
    "\n",
    "    # Create the scalp plot\n",
    "    fig = mne.viz.plot_sensors(raw_array.info, show=False)\n",
    "\n",
    "    # Save the scalp plot as an image in the appropriate folder\n",
    "    if target == 'left':\n",
    "        folder = 'left'\n",
    "    elif target == 'right':\n",
    "        folder = 'right'\n",
    "    else:\n",
    "        raise ValueError(\"Unknown target label\")\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    image_path = os.path.join(folder, f'sample_{index}.png')\n",
    "    fig.savefig(image_path)\n",
    "\n",
    "    # Close the figure to free up resources\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Scalp plots saved as images in separate folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f39311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614cfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f664f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cdc2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b803f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>EEG-Fz</th>\n",
       "      <th>EEG-0</th>\n",
       "      <th>EEG-1</th>\n",
       "      <th>EEG-2</th>\n",
       "      <th>EEG-3</th>\n",
       "      <th>EEG-4</th>\n",
       "      <th>EEG-5</th>\n",
       "      <th>EEG-C3</th>\n",
       "      <th>EEG-6</th>\n",
       "      <th>...</th>\n",
       "      <th>EEG-8</th>\n",
       "      <th>EEG-9</th>\n",
       "      <th>EEG-10</th>\n",
       "      <th>EEG-11</th>\n",
       "      <th>EEG-12</th>\n",
       "      <th>EEG-13</th>\n",
       "      <th>EEG-14</th>\n",
       "      <th>EEG-Pz</th>\n",
       "      <th>EEG-15</th>\n",
       "      <th>EEG-16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tongue</td>\n",
       "      <td>-1.681412</td>\n",
       "      <td>2.245496</td>\n",
       "      <td>-0.158350</td>\n",
       "      <td>1.163765</td>\n",
       "      <td>-1.523659</td>\n",
       "      <td>-0.575267</td>\n",
       "      <td>3.299057</td>\n",
       "      <td>3.928189</td>\n",
       "      <td>0.673606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758116</td>\n",
       "      <td>3.441785</td>\n",
       "      <td>0.305517</td>\n",
       "      <td>1.137473</td>\n",
       "      <td>-1.275763</td>\n",
       "      <td>-2.898359</td>\n",
       "      <td>0.656704</td>\n",
       "      <td>-2.010063</td>\n",
       "      <td>-1.613804</td>\n",
       "      <td>-1.942455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tongue</td>\n",
       "      <td>0.420417</td>\n",
       "      <td>0.587559</td>\n",
       "      <td>1.650510</td>\n",
       "      <td>0.970672</td>\n",
       "      <td>1.505904</td>\n",
       "      <td>0.891796</td>\n",
       "      <td>3.838386</td>\n",
       "      <td>2.514392</td>\n",
       "      <td>1.798873</td>\n",
       "      <td>...</td>\n",
       "      <td>1.541586</td>\n",
       "      <td>-0.071620</td>\n",
       "      <td>0.258909</td>\n",
       "      <td>-1.448198</td>\n",
       "      <td>0.142472</td>\n",
       "      <td>-1.968405</td>\n",
       "      <td>-1.733655</td>\n",
       "      <td>-2.935578</td>\n",
       "      <td>-3.125256</td>\n",
       "      <td>-4.674610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tongue</td>\n",
       "      <td>0.551365</td>\n",
       "      <td>1.499758</td>\n",
       "      <td>0.121302</td>\n",
       "      <td>2.859433</td>\n",
       "      <td>2.613414</td>\n",
       "      <td>4.636026</td>\n",
       "      <td>2.162693</td>\n",
       "      <td>1.522294</td>\n",
       "      <td>-0.072132</td>\n",
       "      <td>...</td>\n",
       "      <td>2.649097</td>\n",
       "      <td>-2.137938</td>\n",
       "      <td>-1.612096</td>\n",
       "      <td>-1.610218</td>\n",
       "      <td>-0.410173</td>\n",
       "      <td>-0.274957</td>\n",
       "      <td>-4.776535</td>\n",
       "      <td>-5.099551</td>\n",
       "      <td>-2.798995</td>\n",
       "      <td>-5.862021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tongue</td>\n",
       "      <td>3.054916</td>\n",
       "      <td>-1.807238</td>\n",
       "      <td>1.843603</td>\n",
       "      <td>2.286812</td>\n",
       "      <td>5.995872</td>\n",
       "      <td>6.651295</td>\n",
       "      <td>2.078354</td>\n",
       "      <td>-1.980015</td>\n",
       "      <td>0.136497</td>\n",
       "      <td>...</td>\n",
       "      <td>6.031554</td>\n",
       "      <td>-5.249621</td>\n",
       "      <td>-2.672998</td>\n",
       "      <td>-3.452370</td>\n",
       "      <td>0.189081</td>\n",
       "      <td>1.593829</td>\n",
       "      <td>-6.081577</td>\n",
       "      <td>-5.476860</td>\n",
       "      <td>-2.932163</td>\n",
       "      <td>-6.874095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tongue</td>\n",
       "      <td>2.506710</td>\n",
       "      <td>-2.453101</td>\n",
       "      <td>0.221178</td>\n",
       "      <td>0.127278</td>\n",
       "      <td>4.519931</td>\n",
       "      <td>6.249573</td>\n",
       "      <td>0.309444</td>\n",
       "      <td>-3.358299</td>\n",
       "      <td>-2.023038</td>\n",
       "      <td>...</td>\n",
       "      <td>7.827097</td>\n",
       "      <td>-5.309546</td>\n",
       "      <td>-2.488783</td>\n",
       "      <td>-3.707608</td>\n",
       "      <td>1.447515</td>\n",
       "      <td>4.268278</td>\n",
       "      <td>-4.383690</td>\n",
       "      <td>-4.218426</td>\n",
       "      <td>-1.331932</td>\n",
       "      <td>-5.322692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57883</th>\n",
       "      <td>left</td>\n",
       "      <td>-22.074154</td>\n",
       "      <td>-11.489719</td>\n",
       "      <td>-9.314989</td>\n",
       "      <td>-8.518715</td>\n",
       "      <td>-6.629442</td>\n",
       "      <td>-10.866221</td>\n",
       "      <td>0.232787</td>\n",
       "      <td>-4.618099</td>\n",
       "      <td>-1.263983</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.208619</td>\n",
       "      <td>3.177499</td>\n",
       "      <td>5.344716</td>\n",
       "      <td>3.831044</td>\n",
       "      <td>6.110942</td>\n",
       "      <td>6.047090</td>\n",
       "      <td>10.841636</td>\n",
       "      <td>13.959124</td>\n",
       "      <td>14.028611</td>\n",
       "      <td>19.901132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57884</th>\n",
       "      <td>left</td>\n",
       "      <td>-24.568827</td>\n",
       "      <td>-12.128923</td>\n",
       "      <td>-11.760834</td>\n",
       "      <td>-9.890342</td>\n",
       "      <td>-8.587006</td>\n",
       "      <td>-11.798394</td>\n",
       "      <td>-1.627120</td>\n",
       "      <td>-2.767069</td>\n",
       "      <td>-1.268422</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.068527</td>\n",
       "      <td>4.198450</td>\n",
       "      <td>6.219184</td>\n",
       "      <td>6.805121</td>\n",
       "      <td>6.594785</td>\n",
       "      <td>6.726245</td>\n",
       "      <td>11.667276</td>\n",
       "      <td>14.540623</td>\n",
       "      <td>14.317140</td>\n",
       "      <td>20.970911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57885</th>\n",
       "      <td>left</td>\n",
       "      <td>-25.776214</td>\n",
       "      <td>-8.892950</td>\n",
       "      <td>-10.233846</td>\n",
       "      <td>-10.316478</td>\n",
       "      <td>-10.233846</td>\n",
       "      <td>-11.785078</td>\n",
       "      <td>1.706509</td>\n",
       "      <td>0.420076</td>\n",
       "      <td>-0.327370</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.934116</td>\n",
       "      <td>6.067236</td>\n",
       "      <td>6.623125</td>\n",
       "      <td>6.525469</td>\n",
       "      <td>5.436226</td>\n",
       "      <td>4.005187</td>\n",
       "      <td>12.950124</td>\n",
       "      <td>14.016830</td>\n",
       "      <td>12.670301</td>\n",
       "      <td>18.347509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57886</th>\n",
       "      <td>left</td>\n",
       "      <td>-22.473657</td>\n",
       "      <td>-6.762268</td>\n",
       "      <td>-7.077773</td>\n",
       "      <td>-9.943609</td>\n",
       "      <td>-9.323867</td>\n",
       "      <td>-14.097755</td>\n",
       "      <td>4.374300</td>\n",
       "      <td>2.355445</td>\n",
       "      <td>2.584561</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.270231</td>\n",
       "      <td>8.393230</td>\n",
       "      <td>8.558495</td>\n",
       "      <td>6.117088</td>\n",
       "      <td>3.270033</td>\n",
       "      <td>-0.260616</td>\n",
       "      <td>12.492915</td>\n",
       "      <td>13.217825</td>\n",
       "      <td>9.869342</td>\n",
       "      <td>17.060222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57887</th>\n",
       "      <td>left</td>\n",
       "      <td>-22.389317</td>\n",
       "      <td>-2.429882</td>\n",
       "      <td>-5.137965</td>\n",
       "      <td>-7.515519</td>\n",
       "      <td>-10.313746</td>\n",
       "      <td>-14.111072</td>\n",
       "      <td>6.167624</td>\n",
       "      <td>5.467128</td>\n",
       "      <td>2.717729</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.994485</td>\n",
       "      <td>9.942414</td>\n",
       "      <td>7.666272</td>\n",
       "      <td>7.324475</td>\n",
       "      <td>1.938357</td>\n",
       "      <td>-2.422370</td>\n",
       "      <td>12.333114</td>\n",
       "      <td>11.642008</td>\n",
       "      <td>6.731025</td>\n",
       "      <td>15.386749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57888 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label     EEG-Fz      EEG-0      EEG-1      EEG-2      EEG-3  \\\n",
       "0      tongue  -1.681412   2.245496  -0.158350   1.163765  -1.523659   \n",
       "1      tongue   0.420417   0.587559   1.650510   0.970672   1.505904   \n",
       "2      tongue   0.551365   1.499758   0.121302   2.859433   2.613414   \n",
       "3      tongue   3.054916  -1.807238   1.843603   2.286812   5.995872   \n",
       "4      tongue   2.506710  -2.453101   0.221178   0.127278   4.519931   \n",
       "...       ...        ...        ...        ...        ...        ...   \n",
       "57883    left -22.074154 -11.489719  -9.314989  -8.518715  -6.629442   \n",
       "57884    left -24.568827 -12.128923 -11.760834  -9.890342  -8.587006   \n",
       "57885    left -25.776214  -8.892950 -10.233846 -10.316478 -10.233846   \n",
       "57886    left -22.473657  -6.762268  -7.077773  -9.943609  -9.323867   \n",
       "57887    left -22.389317  -2.429882  -5.137965  -7.515519 -10.313746   \n",
       "\n",
       "           EEG-4     EEG-5    EEG-C3     EEG-6  ...     EEG-8     EEG-9  \\\n",
       "0      -0.575267  3.299057  3.928189  0.673606  ...  0.758116  3.441785   \n",
       "1       0.891796  3.838386  2.514392  1.798873  ...  1.541586 -0.071620   \n",
       "2       4.636026  2.162693  1.522294 -0.072132  ...  2.649097 -2.137938   \n",
       "3       6.651295  2.078354 -1.980015  0.136497  ...  6.031554 -5.249621   \n",
       "4       6.249573  0.309444 -3.358299 -2.023038  ...  7.827097 -5.309546   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "57883 -10.866221  0.232787 -4.618099 -1.263983  ... -2.208619  3.177499   \n",
       "57884 -11.798394 -1.627120 -2.767069 -1.268422  ... -4.068527  4.198450   \n",
       "57885 -11.785078  1.706509  0.420076 -0.327370  ... -4.934116  6.067236   \n",
       "57886 -14.097755  4.374300  2.355445  2.584561  ... -6.270231  8.393230   \n",
       "57887 -14.111072  6.167624  5.467128  2.717729  ... -9.994485  9.942414   \n",
       "\n",
       "         EEG-10    EEG-11    EEG-12    EEG-13     EEG-14     EEG-Pz  \\\n",
       "0      0.305517  1.137473 -1.275763 -2.898359   0.656704  -2.010063   \n",
       "1      0.258909 -1.448198  0.142472 -1.968405  -1.733655  -2.935578   \n",
       "2     -1.612096 -1.610218 -0.410173 -0.274957  -4.776535  -5.099551   \n",
       "3     -2.672998 -3.452370  0.189081  1.593829  -6.081577  -5.476860   \n",
       "4     -2.488783 -3.707608  1.447515  4.268278  -4.383690  -4.218426   \n",
       "...         ...       ...       ...       ...        ...        ...   \n",
       "57883  5.344716  3.831044  6.110942  6.047090  10.841636  13.959124   \n",
       "57884  6.219184  6.805121  6.594785  6.726245  11.667276  14.540623   \n",
       "57885  6.623125  6.525469  5.436226  4.005187  12.950124  14.016830   \n",
       "57886  8.558495  6.117088  3.270033 -0.260616  12.492915  13.217825   \n",
       "57887  7.666272  7.324475  1.938357 -2.422370  12.333114  11.642008   \n",
       "\n",
       "          EEG-15     EEG-16  \n",
       "0      -1.613804  -1.942455  \n",
       "1      -3.125256  -4.674610  \n",
       "2      -2.798995  -5.862021  \n",
       "3      -2.932163  -6.874095  \n",
       "4      -1.331932  -5.322692  \n",
       "...          ...        ...  \n",
       "57883  14.028611  19.901132  \n",
       "57884  14.317140  20.970911  \n",
       "57885  12.670301  18.347509  \n",
       "57886   9.869342  17.060222  \n",
       "57887   6.731025  15.386749  \n",
       "\n",
       "[57888 rows x 23 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(data.columns[[0,1,3]], axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afba67ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:\n",
      "{'foot': 0, 'left': 1, 'right': 2, 'tongue': 3}\n"
     ]
    }
   ],
   "source": [
    "# Convert 'label' column from categorical to numerical using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['label'])\n",
    "# Get the mapping of numerical labels to original class names\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "# Print the label mapping\n",
    "print(\"Label Mapping:\")\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "756f83e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record: 1 \n",
      "  Features: [3.0, -1.68141184986888, 2.245496203015734, -0.1583499508304185, 1.1637654337849652, -1.5236594460227255, -0.5752670181381107, 3.29905690013112, 3.9281885107080425, 0.6736061789772737, 0.9722089434003512, -2.3405915373688813, -2.562196104676573, 0.7581163953234283, 3.4417852655157346, 0.3055172366695805, 1.1374733664772736, -1.2757628114073412, -2.898358965253496, 0.6567041357080438, -2.0100626912150337, -1.6138036767919577, -1.9424545181381103] \n",
      "  Label: 3\n",
      "Record: 2 \n",
      "  Features: [3.0, 0.420416985358392, 0.5875594132430071, 1.6505101343968536, 0.9706723940122384, 1.505903764204546, 0.891796192089161, 3.838385735358393, 2.5143923459353155, 1.7988725142045467, 1.316225278627622, 0.347174797858392, -1.8275547694493008, 1.5415858555507, -0.071620274256993, 0.2589085718968538, -1.4481977982954546, 0.14247227381993, -1.968405130026223, -1.733654529064684, -2.9355776059877616, -3.125256091564685, -4.674610057910839] \n",
      "  Label: 3\n",
      "Record: 3 \n",
      "  Features: [3.0, 0.5513651387674839, 1.4997575666520997, 0.1213020378059453, 2.8594330474213296, 2.613414417613638, 4.6360255954982525, 2.1626932637674847, 1.5222936243444072, -0.0721324573863611, 0.861235932036714, 2.5777323262674843, 2.6002683839597918, 2.649096508959792, -2.1379377458479016, -1.612096399694054, -1.6102183948863626, -0.4101733227709773, -0.2749569766171322, -4.776534500655593, -5.09955132757867, -2.7989954381555933, -5.862021279501747] \n",
      "  Label: 3\n",
      "Record: 4 \n",
      "  Features: [3.0, 3.05491627513112, -1.8072381719842632, 1.8436031741695829, 2.2868123087849668, 5.995871803977274, 6.65129548186189, 2.078353775131122, -1.980014614291956, 0.1364968039772753, -1.0297441815996486, 4.446517837631122, 3.248350770323428, 6.031553895323429, -5.249620984484262, -2.6729983883304183, -3.452370383522724, 0.1890809385926601, 1.5938285347465044, -6.081577114291957, -5.476859566215032, -2.932163051791956, -6.874095143138111] \n",
      "  Label: 3\n"
     ]
    }
   ],
   "source": [
    "#features separation\n",
    "X= data.iloc[:, 0:].values #EEG signals\n",
    "y= data['label'].values #motor imagery classes\n",
    "\n",
    "for n in range(0,4):\n",
    "    print(\"Record:\", str(n+1), \"\\n  Features:\",list(X[n]), \"\\n  Label:\", y[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29914ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into training , validation and test sets \n",
    "X_train, X_temp, y_train, y_temp= train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b828da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features to have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73ae1bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46310, 23)\n",
      "(5789, 23)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf96943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already loaded the data into X_train and X_test\n",
    "# Reshape the data to add a third dimension (channels) for CNN input\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8ebbc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46310, 23, 1)\n",
      "(5789, 23, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9eeb0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42f64d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "42/42 [==============================] - 1s 22ms/step - loss: -69760704.0000 - accuracy: 0.2503 - val_loss: -73909360.0000 - val_accuracy: 0.2462\n",
      "Epoch 2/20\n",
      "42/42 [==============================] - 1s 24ms/step - loss: -70325928.0000 - accuracy: 0.2503 - val_loss: -74512024.0000 - val_accuracy: 0.2462\n",
      "Epoch 3/20\n",
      "42/42 [==============================] - 1s 21ms/step - loss: -70897776.0000 - accuracy: 0.2503 - val_loss: -75116720.0000 - val_accuracy: 0.2462\n",
      "Epoch 4/20\n",
      "42/42 [==============================] - 1s 21ms/step - loss: -71474088.0000 - accuracy: 0.2503 - val_loss: -75728264.0000 - val_accuracy: 0.2462\n",
      "Epoch 5/20\n",
      "42/42 [==============================] - 1s 22ms/step - loss: -72056704.0000 - accuracy: 0.2503 - val_loss: -76341592.0000 - val_accuracy: 0.2462\n",
      "Epoch 6/20\n",
      "42/42 [==============================] - 1s 27ms/step - loss: -72642504.0000 - accuracy: 0.2503 - val_loss: -76968568.0000 - val_accuracy: 0.2462\n",
      "Epoch 7/20\n",
      "42/42 [==============================] - 1s 22ms/step - loss: -73235872.0000 - accuracy: 0.2503 - val_loss: -77594536.0000 - val_accuracy: 0.2462\n",
      "Epoch 8/20\n",
      "42/42 [==============================] - 1s 25ms/step - loss: -73832688.0000 - accuracy: 0.2503 - val_loss: -78228488.0000 - val_accuracy: 0.2462\n",
      "Epoch 9/20\n",
      "42/42 [==============================] - 1s 21ms/step - loss: -74435200.0000 - accuracy: 0.2503 - val_loss: -78867520.0000 - val_accuracy: 0.2462\n",
      "Epoch 10/20\n",
      "42/42 [==============================] - 1s 23ms/step - loss: -75043048.0000 - accuracy: 0.2503 - val_loss: -79510568.0000 - val_accuracy: 0.2462\n",
      "Epoch 11/20\n",
      "42/42 [==============================] - 1s 28ms/step - loss: -75655456.0000 - accuracy: 0.2503 - val_loss: -80159792.0000 - val_accuracy: 0.2462\n",
      "Epoch 12/20\n",
      "42/42 [==============================] - 1s 27ms/step - loss: -76272200.0000 - accuracy: 0.2503 - val_loss: -80815912.0000 - val_accuracy: 0.2462\n",
      "Epoch 13/20\n",
      "42/42 [==============================] - 1s 27ms/step - loss: -76895000.0000 - accuracy: 0.2503 - val_loss: -81474024.0000 - val_accuracy: 0.2462\n",
      "Epoch 14/20\n",
      "42/42 [==============================] - 1s 21ms/step - loss: -77522776.0000 - accuracy: 0.2503 - val_loss: -82134096.0000 - val_accuracy: 0.2462\n",
      "Epoch 15/20\n",
      "42/42 [==============================] - 1s 23ms/step - loss: -78154224.0000 - accuracy: 0.2503 - val_loss: -82804416.0000 - val_accuracy: 0.2462\n",
      "Epoch 16/20\n",
      "42/42 [==============================] - 1s 22ms/step - loss: -78790448.0000 - accuracy: 0.2503 - val_loss: -83483952.0000 - val_accuracy: 0.2462\n",
      "Epoch 17/20\n",
      "42/42 [==============================] - 1s 22ms/step - loss: -79433472.0000 - accuracy: 0.2503 - val_loss: -84161408.0000 - val_accuracy: 0.2462\n",
      "Epoch 18/20\n",
      "42/42 [==============================] - 1s 26ms/step - loss: -80079872.0000 - accuracy: 0.2503 - val_loss: -84845408.0000 - val_accuracy: 0.2462\n",
      "Epoch 19/20\n",
      "42/42 [==============================] - 1s 22ms/step - loss: -80731264.0000 - accuracy: 0.2503 - val_loss: -85533616.0000 - val_accuracy: 0.2462\n",
      "Epoch 20/20\n",
      "42/42 [==============================] - 1s 21ms/step - loss: -81386120.0000 - accuracy: 0.2503 - val_loss: -86230744.0000 - val_accuracy: 0.2462\n",
      "181/181 [==============================] - 0s 1ms/step - loss: -83115648.0000 - accuracy: 0.2467\n",
      "Accuracy: 0.24667473137378693\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=1000, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b6fdb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181/181 [==============================] - 0s 1ms/step - loss: -83115648.0000 - accuracy: 0.2467\n",
      "Test Loss: -83115648.0000, Test Accuracy: 0.2467\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Model Evaluation\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44251b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 21, 64)            256       \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 10, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 640)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                41024     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,345\n",
      "Trainable params: 41,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36df587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a39ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-f4227f41c33c>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.array(data)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7, 376272]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f4227f41c33c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Split the data into training and testing sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_encoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2170\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At least one array required as input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2172\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m    298\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7, 376272]"
     ]
    }
   ],
   "source": [
    "folder_path = r'C:\\Users\\21694\\Path finding in neurogaming\\csv_files'\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(folder_path, file_name))\n",
    "        data.append(df.drop(['patient', 'time', 'label'], axis=1).to_numpy())\n",
    "        labels.extend(df['label'])\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Perform label encoding if needed\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df94c39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-1a71f182fe3f>:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  features = np.array(features)\n",
      "<ipython-input-6-1a71f182fe3f>:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  labels = np.array(labels)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load CSV files and preprocess data\n",
    "data_folder = r'C:\\Users\\21694\\Path finding in neurogaming\\csv_files'\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through files in the folder\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(data_folder, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Assuming your CSV has columns 'feature_column' and 'label_column'\n",
    "        # Adjust this based on your actual column names\n",
    "        feature_data = df[4:].values\n",
    "        label_data = df['label'].values\n",
    "        \n",
    "        features.append(feature_data)\n",
    "        labels.append(label_data)\n",
    "\n",
    "# Convert lists to arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now you can proceed with building and training your CNN model using X_train and y_train\n",
    "# Make sure to adjust the column names and preprocessing steps based on your actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc114507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Load CSV files and preprocess data\n",
    "data_folder = r'C:\\Users\\21694\\Path finding in neurogaming\\csv_files'\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate through files in the folder\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(data_folder, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Assuming your CSV has columns 'feature_column' and 'label_column'\n",
    "        # Adjust this based on your actual column names\n",
    "        feature_data = df[4:].values\n",
    "        label_data = df['label'].values\n",
    "        \n",
    "        all_features.append(feature_data)\n",
    "        all_labels.append(label_data)\n",
    "\n",
    "# Find the minimum length among all feature arrays\n",
    "min_length = min(len(feature_data) for feature_data in all_features)\n",
    "\n",
    "# Trim all feature and label arrays to have the same length\n",
    "features = [feature_data[:min_length] for feature_data in all_features]\n",
    "labels = [label_data[:min_length] for label_data in all_labels]\n",
    "\n",
    "# Convert lists to arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now you can proceed with building and training your CNN model using X_train and y_train\n",
    "# Make sure to adjust the column names and preprocessing steps based on your actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef06ffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'patient'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a48fa52ba9d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mclass_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[0;32m   1146\u001b[0m         \u001b[1;31m# converting the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(chunk_size)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[1;31m# Convert each value according to its column and store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[1;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[1;31m# Convert each value according to its column and store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[1;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mfloatconv\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'0x'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromhex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'patient'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load data from CSV files\n",
    "data_folder = 'C:\\\\Users\\\\21694\\\\Path finding in neurogaming\\\\csv_files'\n",
    "csv_files = [file for file in os.listdir(data_folder) if file.endswith('.csv')]\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    class_name = csv_file.split('.')[0]\n",
    "    data = np.loadtxt(os.path.join(data_folder, csv_file), delimiter=',')\n",
    "    features.extend(data)\n",
    "    labels.extend([class_name] * len(data))\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Preprocess and split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the feature data\n",
    "X_train_normalized = X_train / 255.0\n",
    "X_test_normalized = X_test / 255.0\n",
    "\n",
    "# Encode labels using one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "num_classes = 4\n",
    "y_train_onehot = to_categorical(y_train_encoded, num_classes)\n",
    "y_test_onehot = to_categorical(y_test_encoded, num_classes)\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_normalized, y_train_onehot, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_normalized, y_test_onehot)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f59a5d13",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-aae6ed3d99ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "csv_folder = 'C:/Users/21694/Path finding in neurogaming/csv_files'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in os.listdir(csv_folder):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        # Read the CSV file\n",
    "        csv_path = os.path.join(csv_folder, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Exclude rows with 'patient' in any column\n",
    "        df = df[~df.apply(lambda row: row.astype(str).str.contains('patient').any(), axis=1)]\n",
    "        \n",
    "        # Convert remaining rows to numpy arrays and append to features\n",
    "        features.extend(df.to_numpy())\n",
    "        \n",
    "        # Extract the label from the CSV file name\n",
    "        label = csv_file.split('.')[0]\n",
    "        labels.extend([label] * len(df))\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Perform label encoding and one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_one_hot = np_utils.to_categorical(labels_encoded)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6893d2bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'left'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-10872aeeccdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Convert remaining rows to numpy arrays and append to features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Ensure data is converted to float\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Extract the label from the CSV file name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'left'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "csv_folder = 'C:/Users/21694/Path finding in neurogaming/csv_files'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in os.listdir(csv_folder):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        # Read the CSV file\n",
    "        csv_path = os.path.join(csv_folder, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Exclude rows with 'patient' in any column\n",
    "        df = df[~df.apply(lambda row: row.astype(str).str.contains('patient').any(), axis=1)]\n",
    "        \n",
    "        # Convert remaining rows to numpy arrays and append to features\n",
    "        features.extend(df.to_numpy().astype(float))  # Ensure data is converted to float\n",
    "        \n",
    "        # Extract the label from the CSV file name\n",
    "        label = csv_file.split('.')[0]\n",
    "        labels.extend([label] * len(df))\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Perform label encoding and one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_one_hot = np_utils.to_categorical(labels_encoded)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the features for CNN input\n",
    "X_train = X_train.reshape(X_train.shape[0], 64, 64, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 64, 64, 1)\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf6ca07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fc9afded419c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mlabels_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mlabels_one_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# Split the dataset into train and test sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     66\u001b[0m   \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m   \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m   \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2752\u001b[0m     \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2753\u001b[0m     \"\"\"\n\u001b[1;32m-> 2754\u001b[1;33m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[0;32m   2755\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0;32m   2756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "csv_folder = 'C:/Users/21694/Path finding in neurogaming/csv_files'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in os.listdir(csv_folder):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        # Read the CSV file\n",
    "        csv_path = os.path.join(csv_folder, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Exclude rows with 'patient' in any column\n",
    "        df = df[~df.apply(lambda row: row.astype(str).str.contains('patient').any(), axis=1)]\n",
    "        \n",
    "        # Convert the remaining rows to numeric values (excluding the first column)\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        df = df.dropna()  # Drop rows with NaN values\n",
    "        features.extend(df.values)\n",
    "        \n",
    "        # Extract the label from the CSV file name\n",
    "        label = csv_file.split('.')[0]\n",
    "        labels.extend([label] * len(df))\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Perform label encoding and one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_one_hot = np_utils.to_categorical(labels_encoded)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the features for CNN input\n",
    "X_train = X_train.reshape(X_train.shape[0], 64, 64, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 64, 64, 1)\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a8cea56",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'patient'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-be94702e1042>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mclass_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[0;32m   1146\u001b[0m         \u001b[1;31m# converting the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(chunk_size)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[1;31m# Convert each value according to its column and store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[1;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[1;31m# Convert each value according to its column and store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[1;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mfloatconv\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'0x'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromhex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'patient'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "csv_folder = 'C:/Users/21694/Path finding in neurogaming/csv_files'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    class_name = csv_file.split('.')[0]\n",
    "    data = np.loadtxt(os.path.join(data_folder, csv_file), delimiter=',')\n",
    "    features.extend(data)\n",
    "    labels.extend([class_name] * len(data))\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# Perform label encoding and one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_one_hot = np_utils.to_categorical(labels_encoded)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the features for CNN input\n",
    "X_train = X_train.reshape(X_train.shape[0], 64, 64, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 64, 64, 1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3dc2672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7446882017619624\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"BCICIV_2a_2.csv\")\n",
    "\n",
    "# Select relevant features (EEG channels)\n",
    "features = data.iloc[:, 5:].values\n",
    "\n",
    "# Select labels (right or left)\n",
    "labels = (data[\"label\"] == \"right\").astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a Support Vector Machine (SVM) classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2e6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('BCICIV_2a_2.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(['label'], axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd834314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout  # Importing Dropout\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6a25082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1303/1303 - 9s - loss: 1.3619 - accuracy: 0.3135 - val_loss: 1.3179 - val_accuracy: 0.3617 - 9s/epoch - 7ms/step\n",
      "Epoch 2/50\n",
      "1303/1303 - 2s - loss: 1.3198 - accuracy: 0.3619 - val_loss: 1.2726 - val_accuracy: 0.4105 - 2s/epoch - 1ms/step\n",
      "Epoch 3/50\n",
      "1303/1303 - 2s - loss: 1.2919 - accuracy: 0.3832 - val_loss: 1.2372 - val_accuracy: 0.4323 - 2s/epoch - 1ms/step\n",
      "Epoch 4/50\n",
      "1303/1303 - 2s - loss: 1.2690 - accuracy: 0.3989 - val_loss: 1.2244 - val_accuracy: 0.4396 - 2s/epoch - 1ms/step\n",
      "Epoch 5/50\n",
      "1303/1303 - 2s - loss: 1.2470 - accuracy: 0.4140 - val_loss: 1.1858 - val_accuracy: 0.4608 - 2s/epoch - 1ms/step\n",
      "Epoch 6/50\n",
      "1303/1303 - 2s - loss: 1.2313 - accuracy: 0.4272 - val_loss: 1.1695 - val_accuracy: 0.4792 - 2s/epoch - 1ms/step\n",
      "Epoch 7/50\n",
      "1303/1303 - 2s - loss: 1.2168 - accuracy: 0.4351 - val_loss: 1.1576 - val_accuracy: 0.4891 - 2s/epoch - 1ms/step\n",
      "Epoch 8/50\n",
      "1303/1303 - 2s - loss: 1.1994 - accuracy: 0.4516 - val_loss: 1.1283 - val_accuracy: 0.4973 - 2s/epoch - 1ms/step\n",
      "Epoch 9/50\n",
      "1303/1303 - 2s - loss: 1.1868 - accuracy: 0.4561 - val_loss: 1.1249 - val_accuracy: 0.5036 - 2s/epoch - 1ms/step\n",
      "Epoch 10/50\n",
      "1303/1303 - 2s - loss: 1.1778 - accuracy: 0.4608 - val_loss: 1.1136 - val_accuracy: 0.5098 - 2s/epoch - 1ms/step\n",
      "Epoch 11/50\n",
      "1303/1303 - 2s - loss: 1.1710 - accuracy: 0.4666 - val_loss: 1.1026 - val_accuracy: 0.5161 - 2s/epoch - 1ms/step\n",
      "Epoch 12/50\n",
      "1303/1303 - 2s - loss: 1.1608 - accuracy: 0.4713 - val_loss: 1.0953 - val_accuracy: 0.5234 - 2s/epoch - 1ms/step\n",
      "Epoch 13/50\n",
      "1303/1303 - 2s - loss: 1.1557 - accuracy: 0.4733 - val_loss: 1.0847 - val_accuracy: 0.5344 - 2s/epoch - 1ms/step\n",
      "Epoch 14/50\n",
      "1303/1303 - 2s - loss: 1.1492 - accuracy: 0.4798 - val_loss: 1.0692 - val_accuracy: 0.5414 - 2s/epoch - 1ms/step\n",
      "Epoch 15/50\n",
      "1303/1303 - 2s - loss: 1.1388 - accuracy: 0.4849 - val_loss: 1.0612 - val_accuracy: 0.5500 - 2s/epoch - 1ms/step\n",
      "Epoch 16/50\n",
      "1303/1303 - 2s - loss: 1.1359 - accuracy: 0.4872 - val_loss: 1.0613 - val_accuracy: 0.5511 - 2s/epoch - 1ms/step\n",
      "Epoch 17/50\n",
      "1303/1303 - 2s - loss: 1.1287 - accuracy: 0.4903 - val_loss: 1.0553 - val_accuracy: 0.5491 - 2s/epoch - 1ms/step\n",
      "Epoch 18/50\n",
      "1303/1303 - 2s - loss: 1.1226 - accuracy: 0.4902 - val_loss: 1.0414 - val_accuracy: 0.5627 - 2s/epoch - 1ms/step\n",
      "Epoch 19/50\n",
      "1303/1303 - 2s - loss: 1.1181 - accuracy: 0.4944 - val_loss: 1.0323 - val_accuracy: 0.5673 - 2s/epoch - 1ms/step\n",
      "Epoch 20/50\n",
      "1303/1303 - 2s - loss: 1.1160 - accuracy: 0.4981 - val_loss: 1.0322 - val_accuracy: 0.5571 - 2s/epoch - 1ms/step\n",
      "Epoch 21/50\n",
      "1303/1303 - 2s - loss: 1.1089 - accuracy: 0.5056 - val_loss: 1.0342 - val_accuracy: 0.5653 - 2s/epoch - 1ms/step\n",
      "Epoch 22/50\n",
      "1303/1303 - 2s - loss: 1.1114 - accuracy: 0.5015 - val_loss: 1.0322 - val_accuracy: 0.5619 - 2s/epoch - 1ms/step\n",
      "Epoch 23/50\n",
      "1303/1303 - 2s - loss: 1.1031 - accuracy: 0.5055 - val_loss: 1.0211 - val_accuracy: 0.5712 - 2s/epoch - 1ms/step\n",
      "Epoch 24/50\n",
      "1303/1303 - 2s - loss: 1.0984 - accuracy: 0.5087 - val_loss: 1.0271 - val_accuracy: 0.5673 - 2s/epoch - 1ms/step\n",
      "Epoch 25/50\n",
      "1303/1303 - 2s - loss: 1.0944 - accuracy: 0.5091 - val_loss: 1.0150 - val_accuracy: 0.5757 - 2s/epoch - 1ms/step\n",
      "Epoch 26/50\n",
      "1303/1303 - 2s - loss: 1.0920 - accuracy: 0.5110 - val_loss: 1.0059 - val_accuracy: 0.5863 - 2s/epoch - 1ms/step\n",
      "Epoch 27/50\n",
      "1303/1303 - 2s - loss: 1.0870 - accuracy: 0.5145 - val_loss: 1.0042 - val_accuracy: 0.5787 - 2s/epoch - 1ms/step\n",
      "Epoch 28/50\n",
      "1303/1303 - 2s - loss: 1.0880 - accuracy: 0.5125 - val_loss: 1.0063 - val_accuracy: 0.5772 - 2s/epoch - 1ms/step\n",
      "Epoch 29/50\n",
      "1303/1303 - 2s - loss: 1.0865 - accuracy: 0.5135 - val_loss: 1.0108 - val_accuracy: 0.5677 - 2s/epoch - 1ms/step\n",
      "Epoch 30/50\n",
      "1303/1303 - 2s - loss: 1.0863 - accuracy: 0.5140 - val_loss: 1.0057 - val_accuracy: 0.5817 - 2s/epoch - 1ms/step\n",
      "Epoch 31/50\n",
      "1303/1303 - 2s - loss: 1.0870 - accuracy: 0.5151 - val_loss: 1.0040 - val_accuracy: 0.5826 - 2s/epoch - 1ms/step\n",
      "Epoch 32/50\n",
      "1303/1303 - 2s - loss: 1.0780 - accuracy: 0.5184 - val_loss: 0.9901 - val_accuracy: 0.5863 - 2s/epoch - 1ms/step\n",
      "Epoch 33/50\n",
      "1303/1303 - 2s - loss: 1.0780 - accuracy: 0.5169 - val_loss: 0.9960 - val_accuracy: 0.5930 - 2s/epoch - 1ms/step\n",
      "Epoch 34/50\n",
      "1303/1303 - 2s - loss: 1.0817 - accuracy: 0.5176 - val_loss: 0.9936 - val_accuracy: 0.5925 - 2s/epoch - 1ms/step\n",
      "Epoch 35/50\n",
      "1303/1303 - 2s - loss: 1.0754 - accuracy: 0.5173 - val_loss: 0.9909 - val_accuracy: 0.5850 - 2s/epoch - 1ms/step\n",
      "Epoch 36/50\n",
      "1303/1303 - 2s - loss: 1.0733 - accuracy: 0.5223 - val_loss: 0.9831 - val_accuracy: 0.5930 - 2s/epoch - 1ms/step\n",
      "Epoch 37/50\n",
      "1303/1303 - 2s - loss: 1.0690 - accuracy: 0.5234 - val_loss: 0.9829 - val_accuracy: 0.5943 - 2s/epoch - 1ms/step\n",
      "Epoch 38/50\n",
      "1303/1303 - 2s - loss: 1.0720 - accuracy: 0.5213 - val_loss: 0.9790 - val_accuracy: 0.5934 - 2s/epoch - 1ms/step\n",
      "Epoch 39/50\n",
      "1303/1303 - 2s - loss: 1.0675 - accuracy: 0.5262 - val_loss: 0.9846 - val_accuracy: 0.5830 - 2s/epoch - 1ms/step\n",
      "Epoch 40/50\n",
      "1303/1303 - 2s - loss: 1.0668 - accuracy: 0.5266 - val_loss: 0.9774 - val_accuracy: 0.5876 - 2s/epoch - 1ms/step\n",
      "Epoch 41/50\n",
      "1303/1303 - 2s - loss: 1.0661 - accuracy: 0.5251 - val_loss: 0.9850 - val_accuracy: 0.5932 - 2s/epoch - 1ms/step\n",
      "Epoch 42/50\n",
      "1303/1303 - 2s - loss: 1.0654 - accuracy: 0.5251 - val_loss: 0.9831 - val_accuracy: 0.5899 - 2s/epoch - 1ms/step\n",
      "Epoch 43/50\n",
      "1303/1303 - 2s - loss: 1.0607 - accuracy: 0.5325 - val_loss: 0.9695 - val_accuracy: 0.5940 - 2s/epoch - 1ms/step\n",
      "Epoch 44/50\n",
      "1303/1303 - 2s - loss: 1.0566 - accuracy: 0.5331 - val_loss: 0.9750 - val_accuracy: 0.5906 - 2s/epoch - 1ms/step\n",
      "Epoch 45/50\n",
      "1303/1303 - 2s - loss: 1.0589 - accuracy: 0.5247 - val_loss: 0.9762 - val_accuracy: 0.5975 - 2s/epoch - 1ms/step\n",
      "Epoch 46/50\n",
      "1303/1303 - 2s - loss: 1.0533 - accuracy: 0.5348 - val_loss: 0.9612 - val_accuracy: 0.5997 - 2s/epoch - 1ms/step\n",
      "Epoch 47/50\n",
      "1303/1303 - 2s - loss: 1.0554 - accuracy: 0.5324 - val_loss: 0.9702 - val_accuracy: 0.5819 - 2s/epoch - 2ms/step\n",
      "Epoch 48/50\n",
      "1303/1303 - 2s - loss: 1.0535 - accuracy: 0.5305 - val_loss: 0.9696 - val_accuracy: 0.5908 - 2s/epoch - 1ms/step\n",
      "Epoch 49/50\n",
      "1303/1303 - 2s - loss: 1.0538 - accuracy: 0.5342 - val_loss: 0.9586 - val_accuracy: 0.6053 - 2s/epoch - 1ms/step\n",
      "Epoch 50/50\n",
      "1303/1303 - 2s - loss: 1.0528 - accuracy: 0.5334 - val_loss: 0.9582 - val_accuracy: 0.5997 - 2s/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a699579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362/362 [==============================] - 0s 1ms/step - loss: 0.9696 - accuracy: 0.5916\n",
      "Test accuracy: 0.5916\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e44ad265",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-adfe649ffb3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Assuming `X_new` contains new data for prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_new_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_new' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming `X_new` contains new data for prediction\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "predictions = model.predict(X_new_scaled)\n",
    "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4abe078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1303/1303 - 7s - loss: 1.3660 - accuracy: 0.3075 - val_loss: 1.3287 - val_accuracy: 0.3477 - 7s/epoch - 5ms/step\n",
      "Epoch 2/50\n",
      "1303/1303 - 2s - loss: 1.3258 - accuracy: 0.3528 - val_loss: 1.2814 - val_accuracy: 0.3941 - 2s/epoch - 1ms/step\n",
      "Epoch 3/50\n",
      "1303/1303 - 2s - loss: 1.2946 - accuracy: 0.3757 - val_loss: 1.2520 - val_accuracy: 0.4075 - 2s/epoch - 1ms/step\n",
      "Epoch 4/50\n",
      "1303/1303 - 2s - loss: 1.2748 - accuracy: 0.3921 - val_loss: 1.2227 - val_accuracy: 0.4355 - 2s/epoch - 1ms/step\n",
      "Epoch 5/50\n",
      "1303/1303 - 2s - loss: 1.2546 - accuracy: 0.4081 - val_loss: 1.2091 - val_accuracy: 0.4433 - 2s/epoch - 1ms/step\n",
      "Epoch 6/50\n",
      "1303/1303 - 2s - loss: 1.2366 - accuracy: 0.4223 - val_loss: 1.1799 - val_accuracy: 0.4634 - 2s/epoch - 1ms/step\n",
      "Epoch 7/50\n",
      "1303/1303 - 2s - loss: 1.2217 - accuracy: 0.4321 - val_loss: 1.1621 - val_accuracy: 0.4779 - 2s/epoch - 1ms/step\n",
      "Epoch 8/50\n",
      "1303/1303 - 2s - loss: 1.2054 - accuracy: 0.4433 - val_loss: 1.1410 - val_accuracy: 0.4908 - 2s/epoch - 1ms/step\n",
      "Epoch 9/50\n",
      "1303/1303 - 2s - loss: 1.1921 - accuracy: 0.4522 - val_loss: 1.1307 - val_accuracy: 0.5062 - 2s/epoch - 1ms/step\n",
      "Epoch 10/50\n",
      "1303/1303 - 2s - loss: 1.1818 - accuracy: 0.4593 - val_loss: 1.1107 - val_accuracy: 0.5152 - 2s/epoch - 2ms/step\n",
      "Epoch 11/50\n",
      "1303/1303 - 2s - loss: 1.1706 - accuracy: 0.4676 - val_loss: 1.0995 - val_accuracy: 0.5245 - 2s/epoch - 1ms/step\n",
      "Epoch 12/50\n",
      "1303/1303 - 2s - loss: 1.1607 - accuracy: 0.4733 - val_loss: 1.0991 - val_accuracy: 0.5228 - 2s/epoch - 1ms/step\n",
      "Epoch 13/50\n",
      "1303/1303 - 2s - loss: 1.1577 - accuracy: 0.4737 - val_loss: 1.0928 - val_accuracy: 0.5249 - 2s/epoch - 1ms/step\n",
      "Epoch 14/50\n",
      "1303/1303 - 2s - loss: 1.1478 - accuracy: 0.4788 - val_loss: 1.0719 - val_accuracy: 0.5334 - 2s/epoch - 1ms/step\n",
      "Epoch 15/50\n",
      "1303/1303 - 2s - loss: 1.1422 - accuracy: 0.4851 - val_loss: 1.0606 - val_accuracy: 0.5394 - 2s/epoch - 1ms/step\n",
      "Epoch 16/50\n",
      "1303/1303 - 2s - loss: 1.1357 - accuracy: 0.4866 - val_loss: 1.0654 - val_accuracy: 0.5429 - 2s/epoch - 2ms/step\n",
      "Epoch 17/50\n",
      "1303/1303 - 2s - loss: 1.1237 - accuracy: 0.4919 - val_loss: 1.0477 - val_accuracy: 0.5552 - 2s/epoch - 1ms/step\n",
      "Epoch 18/50\n",
      "1303/1303 - 2s - loss: 1.1246 - accuracy: 0.4949 - val_loss: 1.0453 - val_accuracy: 0.5573 - 2s/epoch - 1ms/step\n",
      "Epoch 19/50\n",
      "1303/1303 - 2s - loss: 1.1207 - accuracy: 0.4972 - val_loss: 1.0434 - val_accuracy: 0.5509 - 2s/epoch - 1ms/step\n",
      "Epoch 20/50\n",
      "1303/1303 - 2s - loss: 1.1152 - accuracy: 0.5018 - val_loss: 1.0432 - val_accuracy: 0.5588 - 2s/epoch - 1ms/step\n",
      "Epoch 21/50\n",
      "1303/1303 - 2s - loss: 1.1073 - accuracy: 0.5024 - val_loss: 1.0346 - val_accuracy: 0.5580 - 2s/epoch - 1ms/step\n",
      "Epoch 22/50\n",
      "1303/1303 - 2s - loss: 1.1109 - accuracy: 0.5001 - val_loss: 1.0278 - val_accuracy: 0.5694 - 2s/epoch - 1ms/step\n",
      "Epoch 23/50\n",
      "1303/1303 - 2s - loss: 1.1021 - accuracy: 0.5042 - val_loss: 1.0240 - val_accuracy: 0.5677 - 2s/epoch - 1ms/step\n",
      "Epoch 24/50\n",
      "1303/1303 - 2s - loss: 1.1018 - accuracy: 0.5034 - val_loss: 1.0173 - val_accuracy: 0.5757 - 2s/epoch - 2ms/step\n",
      "Epoch 25/50\n",
      "1303/1303 - 2s - loss: 1.0978 - accuracy: 0.5088 - val_loss: 1.0215 - val_accuracy: 0.5662 - 2s/epoch - 1ms/step\n",
      "Epoch 26/50\n",
      "1303/1303 - 2s - loss: 1.0913 - accuracy: 0.5146 - val_loss: 1.0051 - val_accuracy: 0.5832 - 2s/epoch - 1ms/step\n",
      "Epoch 27/50\n",
      "1303/1303 - 2s - loss: 1.0929 - accuracy: 0.5137 - val_loss: 1.0113 - val_accuracy: 0.5740 - 2s/epoch - 1ms/step\n",
      "Epoch 28/50\n",
      "1303/1303 - 2s - loss: 1.0886 - accuracy: 0.5130 - val_loss: 1.0032 - val_accuracy: 0.5776 - 2s/epoch - 1ms/step\n",
      "Epoch 29/50\n",
      "1303/1303 - 2s - loss: 1.0866 - accuracy: 0.5145 - val_loss: 1.0026 - val_accuracy: 0.5811 - 2s/epoch - 1ms/step\n",
      "Epoch 30/50\n",
      "1303/1303 - 2s - loss: 1.0854 - accuracy: 0.5138 - val_loss: 1.0014 - val_accuracy: 0.5891 - 2s/epoch - 1ms/step\n",
      "Epoch 31/50\n",
      "1303/1303 - 2s - loss: 1.0839 - accuracy: 0.5165 - val_loss: 0.9995 - val_accuracy: 0.5871 - 2s/epoch - 1ms/step\n",
      "Epoch 32/50\n",
      "1303/1303 - 2s - loss: 1.0820 - accuracy: 0.5181 - val_loss: 0.9874 - val_accuracy: 0.5910 - 2s/epoch - 1ms/step\n",
      "Epoch 33/50\n",
      "1303/1303 - 2s - loss: 1.0752 - accuracy: 0.5212 - val_loss: 0.9939 - val_accuracy: 0.5755 - 2s/epoch - 1ms/step\n",
      "Epoch 34/50\n",
      "1303/1303 - 2s - loss: 1.0739 - accuracy: 0.5220 - val_loss: 0.9994 - val_accuracy: 0.5770 - 2s/epoch - 1ms/step\n",
      "Epoch 35/50\n",
      "1303/1303 - 2s - loss: 1.0704 - accuracy: 0.5238 - val_loss: 0.9920 - val_accuracy: 0.5858 - 2s/epoch - 1ms/step\n",
      "Epoch 36/50\n",
      "1303/1303 - 2s - loss: 1.0742 - accuracy: 0.5239 - val_loss: 0.9936 - val_accuracy: 0.5791 - 2s/epoch - 1ms/step\n",
      "Epoch 37/50\n",
      "1303/1303 - 2s - loss: 1.0679 - accuracy: 0.5255 - val_loss: 0.9781 - val_accuracy: 0.5841 - 2s/epoch - 1ms/step\n",
      "Epoch 38/50\n",
      "1303/1303 - 2s - loss: 1.0667 - accuracy: 0.5266 - val_loss: 0.9895 - val_accuracy: 0.5845 - 2s/epoch - 1ms/step\n",
      "Epoch 39/50\n",
      "1303/1303 - 2s - loss: 1.0665 - accuracy: 0.5294 - val_loss: 0.9767 - val_accuracy: 0.5914 - 2s/epoch - 1ms/step\n",
      "Epoch 40/50\n",
      "1303/1303 - 2s - loss: 1.0651 - accuracy: 0.5275 - val_loss: 0.9898 - val_accuracy: 0.5876 - 2s/epoch - 1ms/step\n",
      "Epoch 41/50\n",
      "1303/1303 - 2s - loss: 1.0644 - accuracy: 0.5266 - val_loss: 0.9761 - val_accuracy: 0.5919 - 2s/epoch - 1ms/step\n",
      "Epoch 42/50\n",
      "1303/1303 - 2s - loss: 1.0606 - accuracy: 0.5280 - val_loss: 0.9788 - val_accuracy: 0.5837 - 2s/epoch - 1ms/step\n",
      "Epoch 43/50\n",
      "1303/1303 - 2s - loss: 1.0625 - accuracy: 0.5298 - val_loss: 0.9753 - val_accuracy: 0.5966 - 2s/epoch - 1ms/step\n",
      "Epoch 44/50\n",
      "1303/1303 - 2s - loss: 1.0628 - accuracy: 0.5286 - val_loss: 0.9684 - val_accuracy: 0.5930 - 2s/epoch - 1ms/step\n",
      "Epoch 45/50\n",
      "1303/1303 - 2s - loss: 1.0613 - accuracy: 0.5315 - val_loss: 0.9733 - val_accuracy: 0.6014 - 2s/epoch - 1ms/step\n",
      "Epoch 46/50\n",
      "1303/1303 - 2s - loss: 1.0558 - accuracy: 0.5349 - val_loss: 0.9748 - val_accuracy: 0.5988 - 2s/epoch - 1ms/step\n",
      "Epoch 47/50\n",
      "1303/1303 - 2s - loss: 1.0577 - accuracy: 0.5333 - val_loss: 0.9739 - val_accuracy: 0.5921 - 2s/epoch - 1ms/step\n",
      "Epoch 48/50\n",
      "1303/1303 - 2s - loss: 1.0549 - accuracy: 0.5359 - val_loss: 0.9668 - val_accuracy: 0.5927 - 2s/epoch - 1ms/step\n",
      "Epoch 49/50\n",
      "1303/1303 - 2s - loss: 1.0510 - accuracy: 0.5322 - val_loss: 0.9633 - val_accuracy: 0.5947 - 2s/epoch - 1ms/step\n",
      "Epoch 50/50\n",
      "1303/1303 - 2s - loss: 1.0542 - accuracy: 0.5331 - val_loss: 0.9661 - val_accuracy: 0.5906 - 2s/epoch - 1ms/step\n",
      "362/362 [==============================] - 0s 932us/step - loss: 0.9699 - accuracy: 0.5928\n",
      "Test accuracy: 0.5928\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('BCICIV_2a_2.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(['label'], axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Normalize data to [0, 1] range\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "\n",
    "# Reshape to 5x5 image-like representations\n",
    "image_size = 5\n",
    "\n",
    "X_images = []\n",
    "for data_point in X_scaled:\n",
    "    image = data_point.reshape(image_size, image_size, 1)  # Use 1 channel for grayscale\n",
    "    X_images.append(image)\n",
    "X_images = np.array(X_images)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_images, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model layers (similar to previous code)\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model (similar to previous code)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=2)\n",
    "# Evaluate the model (similar to previous code)\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Assuming 'model' is your trained CNN model\n",
    "model.save('cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d363ddd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'left'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0c0dae4ad529>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Assuming you've previously used MinMaxScaler for scaling, you should load it again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Fit the scaler on your original dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mnew_data_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Apply the same scaling to new data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'n_samples_seen_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m         X = self._validate_data(X, reset=first_pass,\n\u001b[0m\u001b[0;32m    397\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                                 force_all_finite=\"allow-nan\")\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         if (\n\u001b[0;32m   2000\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'left'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load and preprocess the new data\n",
    "new_data = pd.read_csv('BCICIV_2a_1.csv')\n",
    "\n",
    "# Assuming you've previously used MinMaxScaler for scaling, you should load it again\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)  # Fit the scaler on your original dataset\n",
    "new_data_scaled = scaler.transform(new_data)  # Apply the same scaling to new data\n",
    "\n",
    "# Reshape the data to match the input shape expected by your model\n",
    "new_data_reshaped = new_data_scaled.reshape(-1, 10, 10, 1)  # Adjust the shape if needed\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Make predictions on the new preprocessed data\n",
    "predictions = model.predict(new_data_reshaped)\n",
    "\n",
    "# Print the predictions or save them as needed\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dab044b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'tongue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-6cd093292af1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Preprocess the new data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mnew_data_preprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Load the pre-trained CNN model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-6cd093292af1>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Your preprocessing steps here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdata_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mdata_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Reshape as needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata_reshaped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'n_samples_seen_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m         X = self._validate_data(X, reset=first_pass,\n\u001b[0m\u001b[0;32m    397\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                                 force_all_finite=\"allow-nan\")\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         if (\n\u001b[0;32m   2000\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'tongue'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'BCICIV_2a_1.csv'\n",
    "new_data = pd.read_csv(csv_file)\n",
    "\n",
    "# Assuming preprocess_data is your preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Your preprocessing steps here\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    data_reshaped = data_scaled.reshape(-1, 10, 10, 1)  # Reshape as needed\n",
    "    return data_reshaped\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data_preprocessed = preprocess_data(new_data)\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = model.predict(new_data_preprocessed)\n",
    "\n",
    "# Process the predictions\n",
    "predicted_classes = []\n",
    "for pred in predictions:\n",
    "    # Map predictions to classes (assuming 4 classes: right, left, tongue, foot)\n",
    "    class_index = np.argmax(pred)  # Get the index of the highest prediction value\n",
    "    if class_index == 0:\n",
    "        predicted_classes.append(\"right\")\n",
    "    elif class_index == 1:\n",
    "        predicted_classes.append(\"left\")\n",
    "    elif class_index == 2:\n",
    "        predicted_classes.append(\"tongue\")\n",
    "    elif class_index == 3:\n",
    "        predicted_classes.append(\"foot\")\n",
    "\n",
    "# Display or use the predicted classes\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc556567",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'tongue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-857b3a6914f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Preprocess the new data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mnew_data_preprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Load the pre-trained CNN model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-857b3a6914f2>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Your preprocessing steps here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdata_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mdata_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Reshape as needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata_reshaped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'n_samples_seen_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m         X = self._validate_data(X, reset=first_pass,\n\u001b[0m\u001b[0;32m    397\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                                 force_all_finite=\"allow-nan\")\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         if (\n\u001b[0;32m   2000\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'tongue'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'BCICIV_2a_1.csv'\n",
    "new_data = pd.read_csv(csv_file)\n",
    "\n",
    "# Assuming preprocess_data is your preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Your preprocessing steps here\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    data_reshaped = data_scaled.reshape(-1, 10, 10, 1)  # Reshape as needed\n",
    "    return data_reshaped\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data_preprocessed = preprocess_data(new_data)\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = model.predict(new_data_preprocessed)\n",
    "\n",
    "# Process the predictions\n",
    "class_labels = ['right', 'left', 'tongue', 'foot']\n",
    "predicted_classes = [class_labels[np.argmax(pred)] for pred in predictions]\n",
    "\n",
    "# Display or use the predicted classes\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b18ff420",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'tongue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0a5ce436de02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Preprocess the new data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mnew_data_preprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Load the pre-trained CNN model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-0a5ce436de02>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Your preprocessing steps here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdata_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mdata_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Reshape as needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata_reshaped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'n_samples_seen_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m         X = self._validate_data(X, reset=first_pass,\n\u001b[0m\u001b[0;32m    397\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                                 force_all_finite=\"allow-nan\")\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         if (\n\u001b[0;32m   2000\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'tongue'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'BCICIV_2a_1.csv'\n",
    "new_data = pd.read_csv(csv_file)\n",
    "\n",
    "# Assuming preprocess_data is your preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Your preprocessing steps here\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    data_reshaped = data_scaled.reshape(-1, 10, 10, 1)  # Reshape as needed\n",
    "    return data_reshaped\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data_preprocessed = preprocess_data(new_data)\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = model.predict(new_data_preprocessed)\n",
    "\n",
    "# Process the predictions\n",
    "class_labels = ['right', 'left', 'tongue', 'foot']\n",
    "predicted_indices = np.argmax(predictions, axis=1)\n",
    "predicted_classes = [class_labels[idx] for idx in predicted_indices]\n",
    "\n",
    "# Display or use the predicted classes\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1e6a0fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 25), found shape=(None, 10, 10, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-91979528f736>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Make predictions on the new data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data_preprocessed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Process the predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 25), found shape=(None, 10, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'BCICIV_2a_1.csv'\n",
    "new_data = pd.read_csv(csv_file)\n",
    "\n",
    "# Assuming preprocess_data is your preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Extract the labels and drop them from the DataFrame\n",
    "    labels = data['label']\n",
    "    data = data.drop(columns=['label'])\n",
    "    \n",
    "    # Your preprocessing steps here\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    data_reshaped = data_scaled.reshape(-1, 10, 10, 1)  # Reshape as needed\n",
    "    return data_reshaped, labels\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data_preprocessed, new_labels = preprocess_data(new_data)\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = model.predict(new_data_preprocessed)\n",
    "\n",
    "# Process the predictions\n",
    "class_labels = ['right', 'left', 'tongue', 'foot']\n",
    "predicted_indices = np.argmax(predictions, axis=1)\n",
    "predicted_classes = [class_labels[idx] for idx in predicted_indices]\n",
    "\n",
    "# Display or use the predicted classes\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bc897d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input shape (10, 10, 1) doesn't match model's input shape (25,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-058f63ae3f8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Make sure the input shape matches the model's input shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnew_data_preprocessed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Input shape {new_data_preprocessed.shape[1:]} doesn't match model's input shape {model.input_shape[1:]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Make predictions on the new data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input shape (10, 10, 1) doesn't match model's input shape (25,)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'BCICIV_2a_1.csv'\n",
    "new_data = pd.read_csv(csv_file)\n",
    "\n",
    "# Assuming preprocess_data is your preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Extract the labels and drop them from the DataFrame\n",
    "    labels = data['label']\n",
    "    data = data.drop(columns=['label'])\n",
    "    \n",
    "    # Your preprocessing steps here\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    data_reshaped = data_scaled.reshape(-1, 10, 10, 1)  # Reshape as needed\n",
    "    return data_reshaped, labels\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data_preprocessed, new_labels = preprocess_data(new_data)\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Make sure the input shape matches the model's input shape\n",
    "if new_data_preprocessed.shape[1:] != model.input_shape[1:]:\n",
    "    raise ValueError(f\"Input shape {new_data_preprocessed.shape[1:]} doesn't match model's input shape {model.input_shape[1:]}\")\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = model.predict(new_data_preprocessed)\n",
    "\n",
    "# Process the predictions as needed\n",
    "class_labels = ['right', 'left', 'tongue', 'foot']\n",
    "predicted_indices = np.argmax(predictions, axis=1)\n",
    "predicted_classes = [class_labels[idx] for idx in predicted_indices]\n",
    "\n",
    "# Display or use the predicted classes\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72204855",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input shape (10, 10, 1) doesn't match model's input shape (25,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-38775926ce21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Make sure the input shape matches the model's input shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnew_data_preprocessed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Input shape {new_data_preprocessed.shape[1:]} doesn't match model's input shape {model.input_shape[1:]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Reshape the preprocessed data to match the expected input shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input shape (10, 10, 1) doesn't match model's input shape (25,)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'BCICIV_2a_1.csv'\n",
    "new_data = pd.read_csv(csv_file)\n",
    "\n",
    "# Assuming preprocess_data is your preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Extract the labels and drop them from the DataFrame\n",
    "    labels = data['label']\n",
    "    data = data.drop(columns=['label'])\n",
    "    \n",
    "    # Your preprocessing steps here\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    data_reshaped = data_scaled.reshape(-1, 10, 10, 1)  # Reshape as needed\n",
    "    return data_reshaped, labels\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data_preprocessed, new_labels = preprocess_data(new_data)\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Make sure the input shape matches the model's input shape\n",
    "if new_data_preprocessed.shape[1:] != model.input_shape[1:]:\n",
    "    raise ValueError(f\"Input shape {new_data_preprocessed.shape[1:]} doesn't match model's input shape {model.input_shape[1:]}\")\n",
    "\n",
    "# Reshape the preprocessed data to match the expected input shape\n",
    "num_samples, height, width, channels = new_data_preprocessed.shape\n",
    "new_data_reshaped = new_data_preprocessed.reshape(num_samples, -1)\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = model.predict(new_data_reshaped)\n",
    "\n",
    "# Process the predictions as needed\n",
    "class_labels = ['right', 'left', 'tongue', 'foot']\n",
    "predicted_indices = np.argmax(predictions, axis=1)\n",
    "predicted_classes = [class_labels[idx] for idx in predicted_indices]\n",
    "\n",
    "# Display or use the predicted classes\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05e0a246",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-22b8cfd0ad87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0mclass_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mpredicted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[0mpredicted_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredicted_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# Display or use the predicted classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-22b8cfd0ad87>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0mclass_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mpredicted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[0mpredicted_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredicted_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# Display or use the predicted classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'BCICIV_2a_1.csv'\n",
    "new_data = pd.read_csv(csv_file)\n",
    "\n",
    "# Assuming preprocess_data is your preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Extract the labels and drop them from the DataFrame\n",
    "    labels = data['label']\n",
    "    data = data.drop(columns=['label'])\n",
    "    \n",
    "    # Your preprocessing steps here\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    data_reshaped = data_scaled.reshape(-1, 10, 10, 1)  # Reshape as needed\n",
    "    return data_reshaped, labels\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data_preprocessed, new_labels = preprocess_data(new_data)\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Get the number of classes from the original model\n",
    "num_classes = model.output_shape[1]\n",
    "\n",
    "# Create a new sequential model with adjusted input shape\n",
    "adjusted_model = Sequential()\n",
    "\n",
    "# Add a convolutional layer with appropriate input shape\n",
    "adjusted_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=new_data_preprocessed.shape[1:]))\n",
    "\n",
    "# Add remaining layers from the original model\n",
    "for layer in model.layers[1:]:\n",
    "    adjusted_model.add(layer)\n",
    "\n",
    "# Compile the adjusted model\n",
    "adjusted_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = adjusted_model.predict(new_data_preprocessed)\n",
    "\n",
    "# Process the predictions as needed\n",
    "class_labels = [0, 1, 2, 3]\n",
    "predicted_indices = np.argmax(predictions, axis=1)\n",
    "predicted_classes = [class_labels[idx] for idx in predicted_indices]\n",
    "\n",
    "# Display or use the predicted classes\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0b2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdd607a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 64)                1664      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,500\n",
      "Trainable params: 18,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Display the model architecture and input/output shapes\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14a24020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 25), found shape=(None, 10, 10, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-ead5ef5ce227>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Train the model using the combined dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombined_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Save the fine-tuned model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 25), found shape=(None, 10, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Load and preprocess the original dataset\n",
    "original_data = pd.read_csv('BCICIV_2a_2.csv')\n",
    "original_data_preprocessed, original_labels = preprocess_data(original_data)\n",
    "\n",
    "# Load and preprocess the additional dataset\n",
    "additional_data = pd.read_csv('BCICIV_2a_1.csv')\n",
    "additional_data_preprocessed, additional_labels = preprocess_data(additional_data)\n",
    "\n",
    "# Ensure both datasets have the same number of samples\n",
    "if original_data_preprocessed.shape[0] != additional_data_preprocessed.shape[0]:\n",
    "    raise ValueError(\"Number of samples in original and additional datasets do not match.\")\n",
    "\n",
    "# Concatenate data and labels to create a unified dataset\n",
    "combined_data = np.concatenate((original_data_preprocessed, additional_data_preprocessed), axis=0)\n",
    "combined_labels = np.concatenate((original_labels, additional_labels), axis=0)\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Get the number of classes from the combined dataset\n",
    "num_classes = len(np.unique(combined_labels))\n",
    "\n",
    "# Add a new output layer to match the number of classes in the combined dataset\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model with an appropriate loss function and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the combined dataset\n",
    "model.fit(combined_data, combined_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save('fine_tuned_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1903013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 25), found shape=(None, 10, 10, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-038d6dff12dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Train the model with the additional data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madditional_data_preprocessed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madditional_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# Save the fine-tuned model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 25), found shape=(None, 10, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Load the additional CSV file\n",
    "additional_csv_file = 'BCICIV_2a_1.csv'\n",
    "additional_data = pd.read_csv(additional_csv_file)\n",
    "\n",
    "# Assuming preprocess_data is your preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Extract the labels and drop them from the DataFrame\n",
    "    labels = data['label']\n",
    "    data = data.drop(columns=['label'])\n",
    "    \n",
    "    # Your preprocessing steps here\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    data_reshaped = data_scaled.reshape(-1, 10, 10, 1)  # Reshape as needed\n",
    "    return data_reshaped, labels\n",
    "\n",
    "# Preprocess the additional data\n",
    "additional_data_preprocessed, additional_labels = preprocess_data(additional_data)\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Get the number of classes from the additional data\n",
    "num_classes = len(np.unique(additional_labels))\n",
    "\n",
    "# Add a new output layer to match the number of classes in the additional data\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model with an appropriate loss function and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with the additional data\n",
    "model.fit(additional_data_preprocessed, additional_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save('fine_tuned_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be1cc392",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 14472\n  y sizes: 57888\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-9ca254c17458>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Fine-tune the model with new data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data_preprocessed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Adjust epochs and batch size as needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Save the updated model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[0;32m   1656\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 14472\n  y sizes: 57888\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Load the CSV file for fine-tuning\n",
    "csv_file_new_data = 'BCICIV_2a_1.csv'\n",
    "new_data = pd.read_csv(csv_file_new_data)\n",
    "\n",
    "# Assuming preprocess_data is your preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Extract the labels and drop them from the DataFrame\n",
    "    labels = data['label']\n",
    "    data = data.drop(columns=['label'])\n",
    "    \n",
    "    # Your preprocessing steps here\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    data_reshaped = data_scaled.reshape(-1, 10, 10, 1)  # Reshape as needed\n",
    "    return data_reshaped, labels\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data_preprocessed, new_labels = preprocess_data(new_data)\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model with new data\n",
    "model.fit(new_data_preprocessed, new_labels, epochs=10, batch_size=32)  # Adjust epochs and batch size as needed\n",
    "\n",
    "# Save the updated model\n",
    "model.save('updated_cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954ac60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d00e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688cdf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841ed30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c67349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed3a01c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1678752 into shape (10,10,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-9b14447468f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Reshape the data if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mnew_data_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_data_preprocessed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Adjust the shape if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Load the trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1678752 into shape (10,10,1)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load the original dataset (assuming you have it saved in 'original_data.csv')\n",
    "original_data = pd.read_csv('BCICIV_2a_2.csv')\n",
    "\n",
    "# Identify categorical columns that need to be encoded\n",
    "categorical_cols = ['label']  # Replace with the actual column names\n",
    "\n",
    "# Define column transformer to apply scaling and encoding together\n",
    "# Here, we assume that all other columns in your dataset are numeric and need scaling\n",
    "# Adjust this based on your dataset's structure\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), ~original_data.columns.isin(categorical_cols)),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on your original dataset\n",
    "preprocessor.fit(original_data)\n",
    "\n",
    "# Load and preprocess the new data\n",
    "new_data = pd.read_csv('BCICIV_2a_1.csv')\n",
    "\n",
    "# Apply the same preprocessing transformations to the new data\n",
    "new_data_preprocessed = preprocessor.transform(new_data)\n",
    "\n",
    "# Reshape the data if needed\n",
    "new_data_reshaped = new_data_preprocessed.reshape(-1, 10, 10, 1)  # Adjust the shape if needed\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "# Make predictions on the new preprocessed data\n",
    "predictions = model.predict(new_data_reshaped)\n",
    "\n",
    "# Print the predictions or save them as needed\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1900234",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 25), found shape=(32, 29)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-9bb817926127>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Make predictions on the new preprocessed and reshaped data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data_reshaped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Print the predictions or save them as needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\21694\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 25), found shape=(32, 29)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load the original dataset (assuming you have it saved in 'original_data.csv')\n",
    "original_data = pd.read_csv('BCICIV_2a_2.csv')\n",
    "\n",
    "# Identify categorical columns that need to be encoded\n",
    "categorical_cols = ['label']  # Replace with the actual column names\n",
    "\n",
    "# Define column transformer to apply scaling and encoding together\n",
    "# Here, we assume that all other columns in your dataset are numeric and need scaling\n",
    "# Adjust this based on your dataset's structure\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), ~original_data.columns.isin(categorical_cols)),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on your original dataset\n",
    "preprocessor.fit(original_data)\n",
    "\n",
    "# Load and preprocess the new data\n",
    "new_data = pd.read_csv('BCICIV_2a_1.csv')\n",
    "\n",
    "new_data_preprocessed = preprocessor.transform(new_data)\n",
    "\n",
    "# Reshape the preprocessed new data to match the expected input shape of the model\n",
    "# Adjust the shape based on your model's input layer shape\n",
    "new_data_reshaped = new_data_preprocessed.reshape(-1, 29)  # Adjust the shape if needed\n",
    "\n",
    "# Make predictions on the new preprocessed and reshaped data\n",
    "predictions = model.predict(new_data_reshaped)\n",
    "\n",
    "# Print the predictions or save them as needed\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81086e71",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1ac7a26f9f2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Load your trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cnn_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Load your model here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Assuming new_data_preprocessed has a shape of (32, 29)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model('cnn_model.h5')  # Load your model here\n",
    "\n",
    "# Assuming new_data_preprocessed has a shape of (32, 29)\n",
    "new_data = pd.read_csv('BCICIV_2a_1.csv')\n",
    "\n",
    "new_data_preprocessed = preprocessor.transform(new_data)\n",
    "\n",
    "# Create the adjusted model with an InputLayer\n",
    "adjusted_model = Sequential([\n",
    "    InputLayer(input_shape=(25,)),  # Adjust the input shape to match new data (29)\n",
    "])\n",
    "\n",
    "# Add layers to the adjusted model to match the loaded model's architecture\n",
    "# You might need to adjust the layer shapes and sizes to match the loaded model\n",
    "adjusted_model.add(Dense(64, activation='relu', input_shape=(29,)))  # Match the first Dense layer\n",
    "adjusted_model.add(Dense(32, activation='relu'))  # Match the second Dense layer\n",
    "adjusted_model.add(Dense(10, activation='softmax'))  # Match the output layer\n",
    "\n",
    "# Load the weights from the loaded model to the adjusted model\n",
    "for i in range(len(adjusted_model.layers)):\n",
    "    adjusted_model.layers[i].set_weights(model.layers[i].get_weights())\n",
    "\n",
    "# Compile the adjusted model\n",
    "adjusted_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Now you can use adjusted_model for predictions on your new data\n",
    "new_data_preprocessed = preprocess_data(new_data)  # Preprocess your new data\n",
    "predictions = adjusted_model.predict(new_data_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fae4bb98",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1273536 into shape (57744,22,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fb1ca58d153a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Reshape X to (num_epochs, num_time_steps, num_features)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mX_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_reshaped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m57744\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m22\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m46310\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m22\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11578\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m22\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1273536 into shape (57744,22,1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(\"BCICIV_2a_2.csv\")\n",
    "X = data.drop([\"patient\", \"time\", \"label\", \"epoch\"], axis=1).values\n",
    "y = data[\"label\"].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Determine the number of time steps per epoch and the number of epochs\n",
    "num_epochs = len(data[\"epoch\"].unique())\n",
    "num_time_steps = X.shape[0] // num_epochs\n",
    "\n",
    "# Reshape X to (num_epochs, num_time_steps, num_features)\n",
    "X_reshaped = X_reshaped.reshape(57744, 22, 1)\n",
    "X_train = X_train.reshape(46310, 22, 1)\n",
    "X_test = X_test.reshape(11578, 22, 1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(label_encoder.classes_), activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print shapes for debugging\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92461df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (57888, 26)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"BCICIV_2a_2.csv\")\n",
    "print(\"Original data shape:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9fe4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique epochs: 288\n"
     ]
    }
   ],
   "source": [
    "unique_epochs = data[\"epoch\"].nunique()\n",
    "print(\"Number of unique epochs:\", unique_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8c923c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape after preprocessing: (57888, 22)\n",
      "y shape after preprocessing: (57888, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape after preprocessing:\", X.shape)\n",
    "print(\"y shape after preprocessing:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "155d4d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_reshaped shape: (288, 201, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_reshaped shape:\", X_reshaped.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af976b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (46310, 22)\n",
      "y_train shape: (46310, 4)\n",
      "X_test shape: (11578, 22)\n",
      "y_test shape: (11578, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd0b442",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1e9a52cd03f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11f8e5ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbc0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
